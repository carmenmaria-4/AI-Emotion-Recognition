import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os
import sys

# =======================================================
# CONFIGURAÈšII DE MEDIU È˜I CÄ‚I
# =======================================================

# DezactiveazÄƒ optimizÄƒrile oneDNN (Intel) pentru stabilitate sporitÄƒ Ã®n antrenare.
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' 

# ObÈ›ine directorul scriptului, asigurÃ¢nd cÄƒile absolute pentru resurse.
try:
    script_dir = os.path.dirname(os.path.abspath(__file__))
except NameError:
    # Cazul Ã®n care scriptul este rulat Ã®ntr-un mediu fÄƒrÄƒ __file__ (e.g., Jupyter)
    script_dir = os.path.dirname(os.path.abspath(sys.argv[0]))

# --- 1. CONFIGURAÈšIE ---
NEW_DATA_DIR = os.path.join(script_dir, 'new_training_data') 
MODEL_FILENAME = '_mini_XCEPTION.102-0.66.hdf5'
SAVE_FILENAME = 'fine_tuned_model.hdf5'

MODEL_PATH = os.path.join(script_dir, 'models', MODEL_FILENAME) 
SAVE_PATH = os.path.join(script_dir, 'models', SAVE_FILENAME)      

INPUT_SIZE = (64, 64)
BATCH_SIZE = 16 
CUSTOM_LEARNING_RATE = 0.00001 
EPOCHS = 15 # NumÄƒrul de epoci de antrenare

# =======================================================
# 2. ÃNCÄ‚RCAREA È˜I PREGÄ‚TIREA DATELOR
# =======================================================

# Generator pentru setul de antrenare
train_datagen = ImageDataGenerator(
    rescale=1./255, 
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True
)

train_generator = train_datagen.flow_from_directory(
    os.path.join(NEW_DATA_DIR, 'train'),
    target_size=INPUT_SIZE,
    color_mode='grayscale',
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

# Generator pentru setul de validare (doar rescalare, fÄƒrÄƒ augmentare)
validation_datagen = ImageDataGenerator(rescale=1./255)

validation_generator = validation_datagen.flow_from_directory(
    os.path.join(NEW_DATA_DIR, 'validation'),
    target_size=INPUT_SIZE,
    color_mode='grayscale',
    batch_size=BATCH_SIZE,
    class_mode='categorical'
)

# =======================================================
# 3. PREGÄ‚TIREA MODELULUI (FINE-TUNING)
# =======================================================

print(f"\nSe Ã®ncarcÄƒ modelul existent de la: {MODEL_PATH}")
try:
    # ÃncÄƒrcÄƒm modelul fÄƒrÄƒ starea optimizatorului vechi
    model = load_model(MODEL_PATH, compile=False) 
except Exception as e:
    print(f"Eroare fatalÄƒ la Ã®ncÄƒrcarea modelului: {e}")
    sys.exit(1)

# ÃngheaÈ›Äƒ majoritatea straturilor pentru a menÈ›ine cunoÈ™tinÈ›ele de bazÄƒ.
# LÄƒsÄƒm ultimele 4 straturi (clasificatoarele) antrenabile.
for layer in model.layers[:-4]:
    layer.trainable = False
    
print(f"NumÄƒr de straturi Ã®ngheÈ›ate: {len(model.layers) - 4} / {len(model.layers)}")

# ConfigureazÄƒ optimizatorul cu rata de Ã®nvÄƒÈ›are foarte micÄƒ
optimizer = tf.keras.optimizers.Adam(learning_rate=CUSTOM_LEARNING_RATE)

# RecompileazÄƒ modelul cu noul optimizator È™i straturile selectate ca antrenabile.
model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy', 
    metrics=['accuracy']
)

model.summary() 

# =======================================================
# 4. ANTRENAMENTUL PROPRIU-ZIS È˜I SALVAREA
# =======================================================

print("\nğŸš€ Start Fine-Tuning...")
try:
    model.fit(
        train_generator,
        steps_per_epoch=train_generator.samples // BATCH_SIZE,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // BATCH_SIZE,
        epochs=EPOCHS
    )

    # SalveazÄƒ modelul nou
    model.save(SAVE_PATH)
    print(f"\nâœ… Antrenamentul s-a terminat. Modelul nou salvat la: {SAVE_PATH}")

except Exception as e:
    print(f"\nâŒ A apÄƒrut o eroare Ã®n timpul antrenamentului: {e}")
    print("Sugestie: AsigurÄƒ-te cÄƒ ai suficientÄƒ memorie RAM/GPU sau Ã®ncearcÄƒ sÄƒ micÈ™orezi BATCH_SIZE.")